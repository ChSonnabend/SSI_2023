{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jngadiub/ML_course_Pavia_23_WIP/blob/main/PartT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsonEqBekjyy"
   },
   "source": [
    "# Jet Tagging with Set Transformer\n",
    "\n",
    "In this notebook we will see an implementation of the Transformer architecture for sets applied to the jet tagging task. For *sets* it is meant here a point cloud, i.e. a set of nodes without edges. We will instead use Multi-Head Attention to learn which nodes (or particles) have strong pair-wise interaction.\n",
    "\n",
    "The architecture was introduced by [J. Lee at al. (ICML 2019)](https://arxiv.org/abs/1810.00825) --  specifically designed to model interactions among elements in the input set without pre-defined edges. The model consists of an encoder and a decoder, both of which rely on attention mechanisms, as in the original Transformer implementation [by Vaswani](https://arxiv.org/abs/1706.03762). The main difference is that positional encoding is removed plus some other low level adaptions.\n",
    "\n",
    "We will use tensorflow for this implementation.\n",
    "\n",
    "Before you start, choose GPU as a hardware accelerator for this notebook. To do this first go to Edit -> Notebook Settings -> Choose GPU as a hardware accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "A7OS3w5WRSCj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 22:39:36.013909: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 22:39:37.553822: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-08-11 22:39:37.554435: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-08-11 22:39:37.656982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:98:00.0 name: NVIDIA A30 computeCapability: 8.0\n",
      "coreClock: 1.44GHz coreCount: 56 deviceMemorySize: 23.50GiB deviceMemoryBandwidth: 869.04GiB/s\n",
      "2023-08-11 22:39:37.657001: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-08-11 22:39:37.658474: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-08-11 22:39:37.658514: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-08-11 22:39:37.659687: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-08-11 22:39:37.659891: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-08-11 22:39:37.661145: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-08-11 22:39:37.661830: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-08-11 22:39:37.664413: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-08-11 22:39:37.668855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot  as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from networks import TransformerNet,RegNet,ClassNet,OutNet,PoolingByMultiHeadAttention\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "J9ZLcoKpPteG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jetConstituentList', 'jetFeatureNames', 'jetImage', 'jetImageECAL', 'jetImageHCAL', 'jets', 'particleFeatureNames']\n"
     ]
    }
   ],
   "source": [
    "# let's open the file\n",
    "data_dir = 'Data-MLtutorial/JetDataset/'\n",
    "fileIN = data_dir+'jetImage_7_100p_30000_40000.h5'\n",
    "f = h5py.File(fileIN,\"r\")\n",
    "# and see what it contains\n",
    "print(list(f.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Re7oXWWmPxz9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_147247/3009411415.py:17: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  f = h5py.File(data_dir + fileIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending jetImage_7_100p_0_10000.h5\n",
      "Appending jetImage_7_100p_10000_20000.h5\n",
      "Appending jetImage_7_100p_30000_40000.h5\n",
      "Appending jetImage_7_100p_40000_50000.h5\n",
      "Appending jetImage_7_100p_50000_60000.h5\n",
      "Appending jetImage_7_100p_60000_70000.h5\n",
      "Appending jetImage_7_100p_70000_80000.h5\n",
      "Appending jetImage_7_100p_80000_90000.h5\n"
     ]
    }
   ],
   "source": [
    "target_onehot = np.array([])\n",
    "jetList = np.array([])\n",
    "jetImages = np.array([])\n",
    "target_reg = np.array([])\n",
    "features_names = dict()\n",
    "datafiles = ['jetImage_7_100p_0_10000.h5',\n",
    "             'jetImage_7_100p_10000_20000.h5',\n",
    "             'jetImage_7_100p_30000_40000.h5',\n",
    "             'jetImage_7_100p_40000_50000.h5',\n",
    "             'jetImage_7_100p_50000_60000.h5',\n",
    "             'jetImage_7_100p_60000_70000.h5',\n",
    "             'jetImage_7_100p_70000_80000.h5',\n",
    "             'jetImage_7_100p_80000_90000.h5'\n",
    "            ]\n",
    "for i_f,fileIN in enumerate(datafiles):\n",
    "    print(\"Appending %s\" %fileIN)\n",
    "    f = h5py.File(data_dir + fileIN)\n",
    "    jetList_file = np.array(f.get(\"jetConstituentList\"))\n",
    "    target_file = np.array(f.get('jets')[0:,-6:-1])\n",
    "    mass = np.array(f.get(\"jets\")[0:,3])    \n",
    "    jetImages_file = np.array(f.get('jetImage'))\n",
    "    jetList = np.concatenate([jetList, jetList_file], axis=0) if jetList.size else jetList_file\n",
    "    target_onehot = np.concatenate([target_onehot, target_file], axis=0) if target_onehot.size else target_file\n",
    "    jetImages = np.concatenate([jetImages, jetImages_file], axis=0) if jetImages.size else jetImages_file\n",
    "    target_reg = np.concatenate([target_reg,mass],axis=0)\n",
    "    del jetList_file, target_file, jetImages_file, mass\n",
    "    #save particles/nodes features names and their indecies in a dictionary\n",
    "    if i_f==0:\n",
    "      for feat_idx,feat_name in enumerate(list(f['particleFeatureNames'])[:-1]):\n",
    "        features_names[feat_name.decode(\"utf-8\").replace('j1_','')] = feat_idx\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hyP15oxhP5ek"
   },
   "outputs": [],
   "source": [
    "target = np.argmax(target_onehot, axis=1)\n",
    "num_classes = len(np.unique(target))\n",
    "label_names= [\"gluon\", \"quark\", \"W\", \"Z\", \"top\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YfHRopq0P8tW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jets shape :  (80000, 100, 16)\n",
      "Target/Labels shape :  (80000,)\n",
      "Particles/Nodes features :  ['px', 'py', 'pz', 'e', 'erel', 'pt', 'ptrel', 'eta', 'etarel', 'etarot', 'phi', 'phirel', 'phirot', 'deltaR', 'costheta', 'costhetarel']\n"
     ]
    }
   ],
   "source": [
    "print('Jets shape : ',jetList.shape)\n",
    "print('Target/Labels shape : ',target.shape)\n",
    "print('Particles/Nodes features : ',list(features_names.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelCat = [\"gluon\", \"quark\", \"W\", \"Z\", \"top\"]\n",
    "def plot_features(features):\n",
    "    for i,f in enumerate(features.keys()):\n",
    "        for l in range(num_classes):\n",
    "            idxs  = np.where([target==l])\n",
    "            plt.hist(jetList[idxs,:,i].flatten(),50, density=True, histtype='step', fill=False, linewidth=1.5,label=labelCat[l])\n",
    "        plt.yscale('log')    \n",
    "        plt.legend(fontsize=12, frameon=False)\n",
    "        plt.xlabel(f)\n",
    "        plt.ylabel('Prob. Density (a.u.)', fontsize=15)\n",
    "        plt.show()\n",
    "#plot_features(features_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QWtB3vTWP_QY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 100, 7)\n"
     ]
    }
   ],
   "source": [
    "features_to_consider = 'etarel,phirel,pt,e,ptrel,erel,deltaR'.split(',')\n",
    "features_idx = [features_names[name] for name in features_to_consider]\n",
    "jetList = jetList[:,:,features_idx]\n",
    "max_mass = np.max(target_reg)\n",
    "target_reg /= max_mass\n",
    "print(jetList.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2mHCuVm6ZJaY"
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "learning_rate=0.001\n",
    "epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "O7rzMn7wRcrP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method MAB.call of <networks.MAB object at 0x14c6635e48e0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method MAB.call of <networks.MAB object at 0x14c6635e48e0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 22:40:06.477551: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-11 22:40:06.477636: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-08-11 22:40:06.480523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:98:00.0 name: NVIDIA A30 computeCapability: 8.0\n",
      "coreClock: 1.44GHz coreCount: 56 deviceMemorySize: 23.50GiB deviceMemoryBandwidth: 869.04GiB/s\n",
      "2023-08-11 22:40:06.480548: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-08-11 22:40:06.480582: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-08-11 22:40:06.480593: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-08-11 22:40:06.480603: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-08-11 22:40:06.480611: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-08-11 22:40:06.480620: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-08-11 22:40:06.480630: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-08-11 22:40:06.480639: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-08-11 22:40:06.485066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-08-11 22:40:06.485089: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-08-11 22:40:07.784591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-08-11 22:40:07.784613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2023-08-11 22:40:07.784618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2023-08-11 22:40:07.789767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22259 MB memory) -> physical GPU (device: 0, name: NVIDIA A30, pci bus id: 0000:98:00.0, compute capability: 8.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method PoolingByMultiHeadAttention.call of <networks.PoolingByMultiHeadAttention object at 0x14c663377340>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method PoolingByMultiHeadAttention.call of <networks.PoolingByMultiHeadAttention object at 0x14c663377340>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 100, 7)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 100, 128)     1024        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "mab (MAB)                       (None, 100, 128)     99584       time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "mab_1 (MAB)                     (None, 100, 128)     99584       mab[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "mab_2 (MAB)                     (None, 100, 128)     99584       mab_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pooling_by_multi_head_attention (None, 128)          561152      mab_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128)          512         pooling_by_multi_head_attention[0\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128)          512         pooling_by_multi_head_attention[0\n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          16512       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          16512       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 128)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          16512       leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          16512       leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 128)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 128)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           2064        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           2064        leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 16)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 16)           0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Reg (Dense)                     (None, 1)            17          leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Class (Dense)                   (None, 5)            85          leaky_re_lu_5[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 932,230\n",
      "Trainable params: 931,718\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_heads = 8 \n",
    "hidden_dimensions = 128\n",
    "use_pooling_by_mha = False\n",
    "\n",
    "t_net = TransformerNet(num_heads=num_heads, hidden_units=hidden_dimensions)\n",
    "r_net = RegNet(hidden_dimensions)\n",
    "c_net = ClassNet(hidden_dimensions)\n",
    "o_net = OutNet(hidden_dimensions, [r_net])\n",
    "\n",
    "inputs = keras.Input(shape=(100,7), name='input')\n",
    "output = layers.TimeDistributed(layers.Dense(hidden_dimensions))(inputs)\n",
    "output = t_net(output)\n",
    "output = PoolingByMultiHeadAttention(num_heads=num_heads,hidden_units=hidden_dimensions)(output)\n",
    "#output = layers.Lambda(lambda y: tf.reduce_sum(y, axis=1))(output)\n",
    "output = [r_net(output), c_net(output)]\n",
    "\n",
    "model = keras.models.Model(inputs=inputs, outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "G8NI-_bYdSAq"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=[\"mse\", keras.losses.SparseCategoricalCrossentropy(from_logits=True)],\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    metrics=keras.metrics.SparseCategoricalAccuracy(),\n",
    "    loss_weights=[0.5,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "DhAKhgMMcrwa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72000, 100, 7) (8000, 100, 7) (72000,) (8000,) (72000,) (8000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train_class, y_val_class, y_train_reg, y_val_reg, y_train_onehot, y_val_onehot = train_test_split(jetList, target,target_reg, target_onehot, test_size=0.1, shuffle=True)\n",
    "print(X_train.shape, X_val.shape, y_train_class.shape, y_val_class.shape, y_train_reg.shape, y_val_reg.shape)\n",
    "del jetList, target, target_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "lz7rfyeCdNF0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 22:40:08.606213: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-08-11 22:40:08.618633: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2800000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 22:40:10.384349: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1125/1125 [==============================] - 24s 20ms/step - loss: 0.8115 - Reg_loss: 0.0132 - Class_loss: 1.6097 - Reg_sparse_categorical_accuracy: 0.0000e+00 - Class_sparse_categorical_accuracy: 0.2029 - val_loss: 1.5078 - val_Reg_loss: 0.0674 - val_Class_loss: 2.9482 - val_Reg_sparse_categorical_accuracy: 0.0000e+00 - val_Class_sparse_categorical_accuracy: 0.1949\n",
      "Epoch 2/20\n",
      "1125/1125 [==============================] - 22s 19ms/step - loss: 0.8108 - Reg_loss: 0.0119 - Class_loss: 1.6097 - Reg_sparse_categorical_accuracy: 0.0000e+00 - Class_sparse_categorical_accuracy: 0.1985 - val_loss: 0.8672 - val_Reg_loss: 0.0604 - val_Class_loss: 1.6739 - val_Reg_sparse_categorical_accuracy: 0.0000e+00 - val_Class_sparse_categorical_accuracy: 0.1985\n",
      "Epoch 3/20\n",
      "1125/1125 [==============================] - 22s 19ms/step - loss: 0.8106 - Reg_loss: 0.0117 - Class_loss: 1.6095 - Reg_sparse_categorical_accuracy: 0.0000e+00 - Class_sparse_categorical_accuracy: 0.2000 - val_loss: 0.8564 - val_Reg_loss: 0.0688 - val_Class_loss: 1.6440 - val_Reg_sparse_categorical_accuracy: 0.0000e+00 - val_Class_sparse_categorical_accuracy: 0.2024\n",
      "Epoch 4/20\n",
      "1125/1125 [==============================] - 22s 19ms/step - loss: 0.8107 - Reg_loss: 0.0118 - Class_loss: 1.6095 - Reg_sparse_categorical_accuracy: 0.0000e+00 - Class_sparse_categorical_accuracy: 0.1995 - val_loss: 0.8422 - val_Reg_loss: 0.0651 - val_Class_loss: 1.6194 - val_Reg_sparse_categorical_accuracy: 0.0000e+00 - val_Class_sparse_categorical_accuracy: 0.1956\n",
      "Epoch 5/20\n",
      "1125/1125 [==============================] - 22s 19ms/step - loss: 0.8107 - Reg_loss: 0.0119 - Class_loss: 1.6095 - Reg_sparse_categorical_accuracy: 0.0000e+00 - Class_sparse_categorical_accuracy: 0.2024 - val_loss: 0.8362 - val_Reg_loss: 0.0595 - val_Class_loss: 1.6128 - val_Reg_sparse_categorical_accuracy: 0.0000e+00 - val_Class_sparse_categorical_accuracy: 0.1937\n",
      "Epoch 6/20\n",
      "1125/1125 [==============================] - 22s 19ms/step - loss: 0.8107 - Reg_loss: 0.0118 - Class_loss: 1.6096 - Reg_sparse_categorical_accuracy: 0.0000e+00 - Class_sparse_categorical_accuracy: 0.2003 - val_loss: 0.8595 - val_Reg_loss: 0.0679 - val_Class_loss: 1.6511 - val_Reg_sparse_categorical_accuracy: 0.0000e+00 - val_Class_sparse_categorical_accuracy: 0.2004\n",
      "Epoch 7/20\n",
      "1125/1125 [==============================] - 22s 19ms/step - loss: 0.8107 - Reg_loss: 0.0118 - Class_loss: 1.6095 - Reg_sparse_categorical_accuracy: 0.0000e+00 - Class_sparse_categorical_accuracy: 0.2037 - val_loss: 0.8374 - val_Reg_loss: 0.0558 - val_Class_loss: 1.6191 - val_Reg_sparse_categorical_accuracy: 0.0000e+00 - val_Class_sparse_categorical_accuracy: 0.1944\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 8/20\n",
      "1125/1125 [==============================] - 22s 19ms/step - loss: 0.8107 - Reg_loss: 0.0118 - Class_loss: 1.6096 - Reg_sparse_categorical_accuracy: 0.0000e+00 - Class_sparse_categorical_accuracy: 0.2030 - val_loss: 0.8329 - val_Reg_loss: 0.0497 - val_Class_loss: 1.6161 - val_Reg_sparse_categorical_accuracy: 0.0000e+00 - val_Class_sparse_categorical_accuracy: 0.1999\n",
      "Epoch 9/20\n",
      "  88/1125 [=>............................] - ETA: 19s - loss: 0.8110 - Reg_loss: 0.0117 - Class_loss: 1.6102 - Reg_sparse_categorical_accuracy: 0.0000e+00 - Class_sparse_categorical_accuracy: 0.1949"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43my_train_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train_class\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[43my_val_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_val_class\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mTerminateOnNaN\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                   \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf_gpu/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1105\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1103\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[0;32m-> 1105\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m   1107\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tf_gpu/lib/python3.9/site-packages/tensorflow/python/keras/callbacks.py:454\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03mArguments:\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 454\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf_gpu/lib/python3.9/site-packages/tensorflow/python/keras/callbacks.py:296\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    294\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 296\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(hook))\n",
      "File \u001b[0;32m~/.conda/envs/tf_gpu/lib/python3.9/site-packages/tensorflow/python/keras/callbacks.py:316\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    313\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[1;32m    314\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 316\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    319\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/.conda/envs/tf_gpu/lib/python3.9/site-packages/tensorflow/python/keras/callbacks.py:356\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(callback, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_supports_tf_logs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 356\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m numpy_logs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Only convert once.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tf_gpu/lib/python3.9/site-packages/tensorflow/python/keras/callbacks.py:1020\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1020\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf_gpu/lib/python3.9/site-packages/tensorflow/python/keras/callbacks.py:1084\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1083\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1084\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1085\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/tf_gpu/lib/python3.9/site-packages/tensorflow/python/keras/utils/tf_utils.py:514\u001b[0m, in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[1;32m    512\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t  \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m--> 514\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf_gpu/lib/python3.9/site-packages/tensorflow/python/util/nest.py:659\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    656\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    660\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/.conda/envs/tf_gpu/lib/python3.9/site-packages/tensorflow/python/util/nest.py:659\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    655\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    656\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 659\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    660\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/.conda/envs/tf_gpu/lib/python3.9/site-packages/tensorflow/python/keras/utils/tf_utils.py:510\u001b[0m, in \u001b[0;36mto_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    509\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, ops\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 510\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[1;32m    512\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/.conda/envs/tf_gpu/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1071\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \n\u001b[1;32m   1050\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1071\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/.conda/envs/tf_gpu/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1037\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1036\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1038\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m     six\u001b[38;5;241m.\u001b[39mraise_from(core\u001b[38;5;241m.\u001b[39m_status_to_exception(e\u001b[38;5;241m.\u001b[39mcode, e\u001b[38;5;241m.\u001b[39mmessage), \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(x=X_train, \n",
    "                    y=[y_train_reg,y_train_class], \n",
    "                    validation_data=(X_val,[y_val_reg,y_val_class]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    callbacks = [EarlyStopping(monitor='val_loss', patience=10, verbose=1),\n",
    "                                  ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1),\n",
    "                                  TerminateOnNaN()]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Egcr8vMhp-2v"
   },
   "source": [
    "We can now plot the validation and training loss evolution over the epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjTOMuzAqGEr"
   },
   "outputs": [],
   "source": [
    "keys = [\"loss\",\"Reg_loss\",\"Class_loss\",\"Class_sparse_categorical_accuracy\"]\n",
    "fig,axes = plt.subplots(2,2, figsize=(20,20))\n",
    "for ax, key in zip(axes.flat, keys):\n",
    "    ax.plot([i for i in range(len(history.history[key]))],history.history[key],label=key)\n",
    "    ax.plot([i for i in range(len(history.history[key]))],history.history[\"val_\"+key],label=\"val_\"+key)\n",
    "    ax.legend()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwrPPStDrS4J"
   },
   "source": [
    "Now we finally evaluate the performance by plotting the ROC curves for the different classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKM0yYFfecJh"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "predict_val = tf.nn.softmax(model.predict(X_val)[1]) # index 0 is the regression predictio\n",
    "df = pd.DataFrame()\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "auc1 = {}\n",
    "\n",
    "plt.figure()\n",
    "for i, label in enumerate(label_names):\n",
    "\n",
    "        df[label] = y_val_onehot[:,i]\n",
    "        df[label + '_pred'] = predict_val[:,i]\n",
    "\n",
    "        fpr[label], tpr[label], threshold = roc_curve(df[label],df[label+'_pred'])\n",
    "\n",
    "        auc1[label] = auc(fpr[label], tpr[label])\n",
    "\n",
    "        plt.plot(fpr[label],tpr[label],label='%s tagger, auc = %.1f%%'%(label,auc1[label]*100.))\n",
    "#plt.semilogy()\n",
    "plt.ylabel(\"sig. efficiency\")\n",
    "plt.xlabel(\"bkg. mistag rate\")\n",
    "plt.ylim(0.000001,1)\n",
    "plt.grid(True)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_reg = model.predict(X_val)[0].flatten()\n",
    "plt.hist(((predict_reg-y_val_reg)/y_val_reg)*max_mass,50,(-300,1000),density=True,histtype=\"step\")\n",
    "plt.xlabel(\"Predicted  - True  / True \")\n",
    "plt.ylabel('Prob. Density (a.u.)', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPn4xtio5MeIQMG/e23naQt",
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
