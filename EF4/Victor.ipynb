{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jngadiub/ML_course_Pavia_23_WIP/blob/main/PartT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsonEqBekjyy"
   },
   "source": [
    "# Jet Tagging with Set Transformer\n",
    "\n",
    "In this notebook we will see an implementation of the Transformer architecture for sets applied to the jet tagging task. For *sets* it is meant here a point cloud, i.e. a set of nodes without edges. We will instead use Multi-Head Attention to learn which nodes (or particles) have strong pair-wise interaction.\n",
    "\n",
    "The architecture was introduced by [J. Lee at al. (ICML 2019)](https://arxiv.org/abs/1810.00825) --  specifically designed to model interactions among elements in the input set without pre-defined edges. The model consists of an encoder and a decoder, both of which rely on attention mechanisms, as in the original Transformer implementation [by Vaswani](https://arxiv.org/abs/1706.03762). The main difference is that positional encoding is removed plus some other low level adaptions.\n",
    "\n",
    "We will use tensorflow for this implementation.\n",
    "\n",
    "Before you start, choose GPU as a hardware accelerator for this notebook. To do this first go to Edit -> Notebook Settings -> Choose GPU as a hardware accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "A7OS3w5WRSCj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot  as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from networks import TransformerNet,RegNet,ClassNet,OutNet,PoolingByMultiHeadAttention\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "J9ZLcoKpPteG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jetConstituentList', 'jetFeatureNames', 'jetImage', 'jetImageECAL', 'jetImageHCAL', 'jets', 'particleFeatureNames']\n"
     ]
    }
   ],
   "source": [
    "# let's open the file\n",
    "data_dir = 'Data-MLtutorial/JetDataset/'\n",
    "fileIN = data_dir+'jetImage_7_100p_30000_40000.h5'\n",
    "f = h5py.File(fileIN,\"r\")\n",
    "# and see what it contains\n",
    "print(list(f.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Re7oXWWmPxz9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending jetImage_7_100p_0_10000.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_151683/3009411415.py:17: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  f = h5py.File(data_dir + fileIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending jetImage_7_100p_10000_20000.h5\n",
      "Appending jetImage_7_100p_30000_40000.h5\n",
      "Appending jetImage_7_100p_40000_50000.h5\n",
      "Appending jetImage_7_100p_50000_60000.h5\n",
      "Appending jetImage_7_100p_60000_70000.h5\n",
      "Appending jetImage_7_100p_70000_80000.h5\n"
     ]
    }
   ],
   "source": [
    "target_onehot = np.array([])\n",
    "jetList = np.array([])\n",
    "jetImages = np.array([])\n",
    "target_reg = np.array([])\n",
    "features_names = dict()\n",
    "datafiles = ['jetImage_7_100p_0_10000.h5',\n",
    "             'jetImage_7_100p_10000_20000.h5',\n",
    "             'jetImage_7_100p_30000_40000.h5',\n",
    "             'jetImage_7_100p_40000_50000.h5',\n",
    "             'jetImage_7_100p_50000_60000.h5',\n",
    "             'jetImage_7_100p_60000_70000.h5',\n",
    "             'jetImage_7_100p_70000_80000.h5',\n",
    "             'jetImage_7_100p_80000_90000.h5'\n",
    "            ]\n",
    "for i_f,fileIN in enumerate(datafiles):\n",
    "    print(\"Appending %s\" %fileIN)\n",
    "    f = h5py.File(data_dir + fileIN)\n",
    "    jetList_file = np.array(f.get(\"jetConstituentList\"))\n",
    "    target_file = np.array(f.get('jets')[0:,-6:-1])\n",
    "    mass = np.array(f.get(\"jets\")[0:,3])    \n",
    "    jetImages_file = np.array(f.get('jetImage'))\n",
    "    jetList = np.concatenate([jetList, jetList_file], axis=0) if jetList.size else jetList_file\n",
    "    target_onehot = np.concatenate([target_onehot, target_file], axis=0) if target_onehot.size else target_file\n",
    "    jetImages = np.concatenate([jetImages, jetImages_file], axis=0) if jetImages.size else jetImages_file\n",
    "    target_reg = np.concatenate([target_reg,mass],axis=0)\n",
    "    del jetList_file, target_file, jetImages_file, mass\n",
    "    #save particles/nodes features names and their indecies in a dictionary\n",
    "    if i_f==0:\n",
    "      for feat_idx,feat_name in enumerate(list(f['particleFeatureNames'])[:-1]):\n",
    "        features_names[feat_name.decode(\"utf-8\").replace('j1_','')] = feat_idx\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hyP15oxhP5ek"
   },
   "outputs": [],
   "source": [
    "target = np.argmax(target_onehot, axis=1)\n",
    "num_classes = len(np.unique(target))\n",
    "label_names= [\"gluon\", \"quark\", \"W\", \"Z\", \"top\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YfHRopq0P8tW"
   },
   "outputs": [],
   "source": [
    "print('Jets shape : ',jetList.shape)\n",
    "print('Target/Labels shape : ',target.shape)\n",
    "print('Particles/Nodes features : ',list(features_names.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelCat = [\"gluon\", \"quark\", \"W\", \"Z\", \"top\"]\n",
    "def plot_features(features):\n",
    "    for i,f in enumerate(features.keys()):\n",
    "        for l in range(num_classes):\n",
    "            idxs  = np.where([target==l])\n",
    "            plt.hist(jetList[idxs,:,i].flatten(),50, density=True, histtype='step', fill=False, linewidth=1.5,label=labelCat[l])\n",
    "        plt.yscale('log')    \n",
    "        plt.legend(fontsize=12, frameon=False)\n",
    "        plt.xlabel(f)\n",
    "        plt.ylabel('Prob. Density (a.u.)', fontsize=15)\n",
    "        plt.show()\n",
    "#plot_features(features_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QWtB3vTWP_QY"
   },
   "outputs": [],
   "source": [
    "#features_to_consider = 'etarel,phirel,pt,e,ptrel,erel,deltaR'.split(',')\n",
    "#features_idx = [features_names[name] for name in features_to_consider]\n",
    "#jetList = jetList[:,:,features_idx]\n",
    "max_mass = np.max(target_reg)\n",
    "target_reg /= max_mass\n",
    "print(jetList.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2mHCuVm6ZJaY"
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "learning_rate=0.001\n",
    "epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7rzMn7wRcrP"
   },
   "outputs": [],
   "source": [
    "num_heads = 8\n",
    "hidden_dimensions = 64\n",
    "\n",
    "t_net = TransformerNet(num_heads=num_heads, hidden_units=hidden_dimensions)\n",
    "r_net = RegNet(hidden_dimensions)\n",
    "c_net = ClassNet(hidden_dimensions)\n",
    "\n",
    "inputs = keras.Input(shape=(100,16), name='input')\n",
    "output = layers.TimeDistributed(layers.Dense(hidden_dimensions))(inputs)\n",
    "output = t_net(output)\n",
    "# output = PoolingByMultiHeadAttention(num_heads=num_heads,hidden_units=hidden_dimensions)(output)\n",
    "output = layers.Lambda(lambda y: tf.reduce_sum(y, axis=1))(output)\n",
    "output = [r_net(output), c_net(output)]\n",
    "# output = o_net(output)\n",
    "\n",
    "model = keras.models.Model(inputs=inputs, outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8NI-_bYdSAq"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=[\"mse\", keras.losses.SparseCategoricalCrossentropy(from_logits=True)],\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    metrics=keras.metrics.SparseCategoricalAccuracy(),\n",
    "    loss_weights=[0.5,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DhAKhgMMcrwa"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train_class, y_val_class, y_train_reg, y_val_reg, y_train_onehot, y_val_onehot = train_test_split(jetList, target,target_reg, target_onehot, test_size=0.1, shuffle=True)\n",
    "print(X_train.shape, X_val.shape, y_train_class.shape, y_val_class.shape, y_train_reg.shape, y_val_reg.shape)\n",
    "del jetList, target, target_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lz7rfyeCdNF0"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x=X_train, \n",
    "                    y=[y_train_reg,y_train_class], \n",
    "                    validation_data=(X_val,[y_val_reg,y_val_class]), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    callbacks = [ EarlyStopping(monitor='val_loss', patience=10, verbose=1),\n",
    "                                  ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1),\n",
    "                                  TerminateOnNaN()]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Egcr8vMhp-2v"
   },
   "source": [
    "We can now plot the validation and training loss evolution over the epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjTOMuzAqGEr"
   },
   "outputs": [],
   "source": [
    "keys = [\"loss\",\"Reg_loss\",\"Class_loss\",\"Class_sparse_categorical_accuracy\"]\n",
    "fig,axes = plt.subplots(2,2, figsize=(20,20))\n",
    "for ax, key in zip(axes.flat, keys):\n",
    "    ax.plot([i for i in range(len(history.history[key]))],history.history[key],label=key)\n",
    "    ax.plot([i for i in range(len(history.history[key]))],history.history[\"val_\"+key],label=\"val_\"+key)\n",
    "    ax.legend()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwrPPStDrS4J"
   },
   "source": [
    "Now we finally evaluate the performance by plotting the ROC curves for the different classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKM0yYFfecJh"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "predict_val = tf.nn.softmax(model.predict(X_val)[1]) # index 0 is the regression predictio\n",
    "df = pd.DataFrame()\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "auc1 = {}\n",
    "\n",
    "plt.figure()\n",
    "for i, label in enumerate(label_names):\n",
    "\n",
    "        df[label] = y_val_onehot[:,i]\n",
    "        df[label + '_pred'] = predict_val[:,i]\n",
    "\n",
    "        fpr[label], tpr[label], threshold = roc_curve(df[label],df[label+'_pred'])\n",
    "\n",
    "        auc1[label] = auc(fpr[label], tpr[label])\n",
    "\n",
    "        plt.plot(fpr[label],tpr[label],label='%s tagger, auc = %.1f%%'%(label,auc1[label]*100.))\n",
    "#plt.semilogy()\n",
    "plt.ylabel(\"sig. efficiency\")\n",
    "plt.xlabel(\"bkg. mistag rate\")\n",
    "plt.ylim(0.000001,1)\n",
    "plt.grid(True)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_reg = model.predict(X_val)[0].flatten()\n",
    "plt.hist(((predict_reg-y_val_reg)/y_val_reg)*max_mass,50,(-200,200),density=True,histtype=\"step\")\n",
    "plt.xlabel(\"Predicted  - True  / True \")\n",
    "plt.ylabel('Prob. Density (a.u.)', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_mat = confusion_matrix(np.argmax(predict_val,axis=1),y_val_class)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(confusion_mat, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "\n",
    "tick_marks = np.arange(num_classes)\n",
    "plt.xticks(tick_marks, range(num_classes))\n",
    "plt.yticks(tick_marks, range(num_classes))\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPn4xtio5MeIQMG/e23naQt",
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
